# ğŸ§  Deep Q-Learning ile CartPole Ajan EÄŸitimi
Bu proje, OpenAI Gymâ€™in klasik kontrol problemlerinden biri olan CartPole-v1 ortamÄ±nda, Deep Q-Learning (DQL) algoritmasÄ±nÄ± kullanarak bir ajan eÄŸitmeyi amaÃ§lamaktadÄ±r. Hedef, bir Ã§ubuÄŸu dik tutmaya Ã§alÄ±ÅŸan bir ajanÄ±n Ã§evreyle etkileÅŸim kurarak optimum politikayÄ± Ã¶ÄŸrenmesini saÄŸlamaktÄ±r.

## ğŸš€ Projenin AmacÄ±
CartPole ortamÄ±nda:

- Ajan, bir arabanÄ±n Ã¼stÃ¼ndeki Ã§ubuÄŸun dÃ¼ÅŸmemesi iÃ§in saÄŸa veya sola hareket eder.

- Ã‡ubuÄŸun mÃ¼mkÃ¼n olan en uzun sÃ¼re dengede tutulmasÄ± hedeflenir.

- Ajan, durum bilgilerine bakarak hangi eylemi seÃ§eceÄŸini Ã¶ÄŸrenir.


## ğŸ›  KullanÄ±lan Teknolojiler
- Python 3.x

- OpenAI Gym â€“ RL ortamÄ±

- TensorFlow / Keras â€“ Derin sinir aÄŸÄ± modeli

- NumPy â€“ SayÄ±sal iÅŸlemler

- TQDM â€“ EÄŸitim ilerlemesi iÃ§in ilerleme Ã§ubuÄŸu

- collections.deque ve random â€“ Deneyim tekrar belleÄŸi


## ğŸ§  Deep Q-Learning BileÅŸenleri
- Experience Replay (Deneyim TekrarÄ±):
AjanÄ±n deneyimlerini hafÄ±zada tutarak rastgele Ã¶rneklerle Ã¶ÄŸrenmesi, korelasyonlarÄ± azaltÄ±r ve Ã¶ÄŸrenmeyi daha kararlÄ± hale getirir.

- Îµ-Greedy PolitikasÄ±:
Ajan baÅŸlangÄ±Ã§ta rastgele hareket ederek keÅŸif yapar (exploration). Zamanla Ã¶ÄŸrendiÄŸi bilgiye gÃ¶re daha bilinÃ§li hareket eder (exploitation).

- Q-Network (Derin Sinir AÄŸÄ±):
Girdi olarak ortamÄ±n durum vektÃ¶rÃ¼nÃ¼ alÄ±r, Ã§Ä±ktÄ± olarak her eylem iÃ§in Q-deÄŸerini (beklenen Ã¶dÃ¼l) tahmin eder.


## ğŸ§© Model Mimarisi
GiriÅŸ KatmanÄ± : 4 nÃ¶ron  â†’ (CartPole gÃ¶zlem uzayÄ±)

Gizli Katman 1: 48 nÃ¶ron â†’ ReLU aktivasyon

Gizli Katman 2: 24 nÃ¶ron â†’ ReLU aktivasyon

Ã‡Ä±kÄ±ÅŸ KatmanÄ± : 2 nÃ¶ron  â†’ Lineer aktivasyon (eylem sayÄ±sÄ±: saÄŸa veya sola hareket)


## âš™ï¸ Kurulum
AÅŸaÄŸÄ±daki komutlarla gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyebilirsiniz:
```bash
pip install numpy tensorflow gym tqdm
```

## ğŸ‹ï¸â€â™‚ï¸ EÄŸitimi BaÅŸlatma
```python
episodes = 5  # EÄŸitim sÃ¼resini artÄ±rmak isterseniz bu deÄŸeri yÃ¼kseltebilirsiniz

for e in range(episodes):
    state = env.reset()[0]
    state = np.reshape(state, [1, 4])
    done = False
    time_t = 0

    while not done:
        action = agent.act(state)
        next_state, reward, done, _, _ = env.step(action)
        next_state = np.reshape(next_state, [1, 4])
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        agent.replay()  # Ajan Ã¶ÄŸrenmesini burada gerÃ§ekleÅŸtirir
        time_t += 1

    print(f"Episode {e+1}/{episodes} finished after {time_t} timesteps")
```

## ğŸ§ª EÄŸitilmiÅŸ AjanÄ± Test Etme
AÅŸaÄŸÄ±daki kod parÃ§asÄ± ile eÄŸitilmiÅŸ ajanÄ± gÃ¶rsel olarak test edebilirsiniz:
```python
trained_model = agent

env = gym.make("CartPole-v1", render_mode="human")  # Ortam insan gÃ¶zÃ¼yle izlenebilir ÅŸekilde baÅŸlatÄ±lÄ±r
state = env.reset()[0]
state = np.reshape(state, [1, 4])

time_t = 0

while True:
    env.render()  # OrtamÄ± ekranda gÃ¶sterir
    action = trained_model.act(state)  # AjanÄ±n kararÄ±
    next_state, reward, done, _, _ = env.step(action)  # Ortamla etkileÅŸim
    next_state = np.reshape(next_state, [1, 4])
    state = next_state
    time_t += 1
    print(f"Time: {time_t}")
    time.sleep(0.5)  # GÃ¶zle izlemek iÃ§in yavaÅŸlatma

    if done:
        break

print("Done")

```

## ğŸ“ˆ GeliÅŸtirme Ã–nerileri
- Double DQN veya Dueling DQN gibi geliÅŸmiÅŸ algoritmalarla deneyin.
- Ortama reward shaping ekleyerek daha iyi performans elde edebilirsiniz.
- EÄŸitim verilerini grafiksel olarak izlemek iÃ§in Matplotlib ile Ã¶dÃ¼l/episode Ã§izimi yapabilirsiniz.

## ğŸ§¾ Lisans
Bu proje **MIT** lisansÄ± ile lisanslanmÄ±ÅŸtÄ±r. Ä°stediÄŸiniz gibi kullanabilir, geliÅŸtirebilir ve paylaÅŸabilirsiniz.
