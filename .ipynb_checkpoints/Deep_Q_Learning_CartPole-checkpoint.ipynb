{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef4a43b-ad57-4400-8779-0181b4271f6f",
   "metadata": {},
   "source": [
    "# ✅Gerekli Kütüphanelerin İçe Aktarılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5188bc64-b7a8-4570-890a-4c152198b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # NumPy, özellikle vektör ve matris işlemleri için kullanılır.\n",
    "import gym  # OpenAI Gym, RL ortamlarını sağlar. Burada CartPole kullanılıyor.\n",
    "from collections import deque  # Deneyim hafızası için çift taraflı kuyruk.\n",
    "from tensorflow.keras.models import Sequential  # Basit katmanlı model tanımı.\n",
    "from tensorflow.keras.layers import Dense  # Yapay sinir ağına tam bağlantılı katmanlar ekler.\n",
    "from tensorflow.keras.optimizers import Adam  # Geri yayılımda kullanılan optimizasyon algoritması.\n",
    "import random  # Rastgele eylem seçimi ve örnekleme için kullanılır.\n",
    "from tqdm import tqdm  # Eğitim döngüsü ilerlemesini görsel olarak takip etmeye yarar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628cdbb-c6fd-4dfe-88ea-32697c820ef0",
   "metadata": {},
   "source": [
    "# ✅ Derin Q-Learning Ajanının Tanımı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3233de3c-3c8e-49d4-80ea-c142174fd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]  # Ortamın gözlem (state) boyutu alınır.\n",
    "        self.action_size = env.action_space.n  # Ortamda yapılabilecek toplam eylem sayısı.\n",
    "\n",
    "        self.gamma = 0.95  # Gelecekteki ödüllerin bugüne indirgenme katsayısı (discount factor).\n",
    "        self.learning_rate = 0.001  # Öğrenme oranı.\n",
    "\n",
    "        self.epsilon = 1  # Başlangıçta tamamen rastgele eylem seçimi (exploration).\n",
    "        self.epsilon_decay = 0.995  # Her bölüm sonunda epsilon değeri yavaşça azaltılır.\n",
    "        self.epsilon_min = 0.01  # Epsilon’un ulaşabileceği en düşük değer.\n",
    "\n",
    "        self.memory = deque(maxlen=1000)  # Deneyimlerin saklanacağı bir kuyruk (Experience Replay).\n",
    "\n",
    "        self.model = self.build_model()  # Sinir ağı modeli oluşturulur.\n",
    "\n",
    "    # Sinir Ağı Modelinin İnşası\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(48, input_dim=self.state_size, activation=\"relu\"))  # İlk katman: Girdi boyutuna göre nöronlar.\n",
    "        \n",
    "        model.add(Dense(24, activation=\"relu\"))  # Orta katman: daha az nöron ile temsil gücü.\n",
    "       \n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))  # Çıkış katmanı: Her aksiyon için bir değer.\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))  # Hata fonksiyonu ve optimizer.\n",
    "        return model\n",
    "\n",
    "    # Hafızaya Deneyim Ekleme Fonksiyonu\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))  # Her adımı belleğe kaydeder.\n",
    "\n",
    "    # Aksiyon Seçme Fonksiyonu (ε-Greedy Politikası)\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return env.action_space.sample()  # Rastgele aksiyon seç (exploration).\n",
    "        \n",
    "        act_values = self.model.predict(state, verbose=0)  # Modelle tahmin yap (exploitation).\n",
    "        return np.argmax(act_values[0])  # En yüksek Q-değerine sahip aksiyon seçilir.\n",
    "\n",
    "    # Replay - Ajanın Öğrenme Fonksiyonu\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return  # Yeterli deneyim yoksa öğrenme yapılmaz.\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)  # Hafızadan rastgele örnekler seçilir.\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if done:\n",
    "                target = reward  # Eğer bölüm bittiyse sadece ödül kullanılır.\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "                # Eğer devam ediyorsa gelecekteki maksimum ödül eklenir.\n",
    "\n",
    "            train_target = self.model.predict(state, verbose=0)\n",
    "            train_target[0][action] = target  # Sadece ilgili aksiyonun Q-değeri güncellenir.\n",
    "\n",
    "            self.model.fit(state, train_target, verbose=0)  # Model eğitilir.\n",
    "\n",
    "        \n",
    "    # Epsilon Güncelleme Fonksiyonu (Exploration Azaltımı)\n",
    "    def adaptiveEGreedy(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay  # Epsilon zamanla azaltılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437fb8ac-2446-4bf1-bfec-b04e16f77091",
   "metadata": {},
   "source": [
    "# ✅ Ana Eğitim Döngüsü - Ajanın Ortamda Öğrenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52add913-24b6-42b0-aa88-4b4408477379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teymur Mammadov\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\Teymur Mammadov\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      " 20%|██        | 1/5 [00:08<00:33,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 18 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")  # Ortam oluşturulur (insan gözlemi için).\n",
    "agent = DQLAgent(env)\n",
    "\n",
    "batch_size = 32  # Mini-batch boyutu.\n",
    "episodes = 5  # Eğitim bölümü sayısı (daha sonra artırılabilir).\n",
    "\n",
    "for e in tqdm(range(episodes)):\n",
    "    state = env.reset()[0]  # Ortam sıfırlanır ve başlangıç durumu alınır.\n",
    "    state = np.reshape(state, [1, 4])  # Modelle uyumlu hale getirilir.\n",
    "\n",
    "    time = 0  # Adım sayacı\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)  # Ajan aksiyon seçer.\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated  # Bölümün bitip bitmediği kontrol edilir.\n",
    "        next_state = np.reshape(next_state, [1, 4])  # Modelle uyumlu hale getirilir.\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)  # Hafızaya kaydedilir.\n",
    "        state = next_state  # Yeni duruma geçilir.\n",
    "\n",
    "        agent.replay(batch_size)  # Öğrenme yapılır.\n",
    "        agent.adaptiveEGreedy()  # Epsilon güncellenir.\n",
    "\n",
    "        time += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {e+1} finished after {time} timesteps\")  # Bölüm sonucu yazdırılır.\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
