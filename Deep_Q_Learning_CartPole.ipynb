{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef4a43b-ad57-4400-8779-0181b4271f6f",
   "metadata": {},
   "source": [
    "# âœ…Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5188bc64-b7a8-4570-890a-4c152198b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # NumPy, Ã¶zellikle vektÃ¶r ve matris iÅŸlemleri iÃ§in kullanÄ±lÄ±r.\n",
    "import gym  # OpenAI Gym, RL ortamlarÄ±nÄ± saÄŸlar. Burada CartPole kullanÄ±lÄ±yor.\n",
    "from collections import deque  # Deneyim hafÄ±zasÄ± iÃ§in Ã§ift taraflÄ± kuyruk.\n",
    "from tensorflow.keras.models import Sequential  # Basit katmanlÄ± model tanÄ±mÄ±.\n",
    "from tensorflow.keras.layers import Dense  # Yapay sinir aÄŸÄ±na tam baÄŸlantÄ±lÄ± katmanlar ekler.\n",
    "from tensorflow.keras.optimizers import Adam  # Geri yayÄ±lÄ±mda kullanÄ±lan optimizasyon algoritmasÄ±.\n",
    "import random  # Rastgele eylem seÃ§imi ve Ã¶rnekleme iÃ§in kullanÄ±lÄ±r.\n",
    "from tqdm import tqdm  # EÄŸitim dÃ¶ngÃ¼sÃ¼ ilerlemesini gÃ¶rsel olarak takip etmeye yarar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628cdbb-c6fd-4dfe-88ea-32697c820ef0",
   "metadata": {},
   "source": [
    "# âœ… Derin Q-Learning AjanÄ±nÄ±n TanÄ±mÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3233de3c-3c8e-49d4-80ea-c142174fd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]  # OrtamÄ±n gÃ¶zlem (state) boyutu alÄ±nÄ±r.\n",
    "        self.action_size = env.action_space.n  # Ortamda yapÄ±labilecek toplam eylem sayÄ±sÄ±.\n",
    "\n",
    "        self.gamma = 0.95  # Gelecekteki Ã¶dÃ¼llerin bugÃ¼ne indirgenme katsayÄ±sÄ± (discount factor).\n",
    "        self.learning_rate = 0.001  # Ã–ÄŸrenme oranÄ±.\n",
    "\n",
    "        self.epsilon = 1  # BaÅŸlangÄ±Ã§ta tamamen rastgele eylem seÃ§imi (exploration).\n",
    "        self.epsilon_decay = 0.995  # Her bÃ¶lÃ¼m sonunda epsilon deÄŸeri yavaÅŸÃ§a azaltÄ±lÄ±r.\n",
    "        self.epsilon_min = 0.01  # Epsilonâ€™un ulaÅŸabileceÄŸi en dÃ¼ÅŸÃ¼k deÄŸer.\n",
    "\n",
    "        self.memory = deque(maxlen=1000)  # Deneyimlerin saklanacaÄŸÄ± bir kuyruk (Experience Replay).\n",
    "\n",
    "        self.model = self.build_model()  # Sinir aÄŸÄ± modeli oluÅŸturulur.\n",
    "\n",
    "    # Sinir AÄŸÄ± Modelinin Ä°nÅŸasÄ±\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(48, input_dim=self.state_size, activation=\"relu\"))  # Ä°lk katman: Girdi boyutuna gÃ¶re nÃ¶ronlar.\n",
    "        \n",
    "        model.add(Dense(24, activation=\"relu\"))  # Orta katman: daha az nÃ¶ron ile temsil gÃ¼cÃ¼.\n",
    "       \n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))  # Ã‡Ä±kÄ±ÅŸ katmanÄ±: Her aksiyon iÃ§in bir deÄŸer.\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))  # Hata fonksiyonu ve optimizer.\n",
    "        return model\n",
    "\n",
    "    # HafÄ±zaya Deneyim Ekleme Fonksiyonu\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))  # Her adÄ±mÄ± belleÄŸe kaydeder.\n",
    "\n",
    "    # Aksiyon SeÃ§me Fonksiyonu (Îµ-Greedy PolitikasÄ±)\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return env.action_space.sample()  # Rastgele aksiyon seÃ§ (exploration).\n",
    "        \n",
    "        act_values = self.model.predict(state, verbose=0)  # Modelle tahmin yap (exploitation).\n",
    "        return np.argmax(act_values[0])  # En yÃ¼ksek Q-deÄŸerine sahip aksiyon seÃ§ilir.\n",
    "\n",
    "    # Replay - AjanÄ±n Ã–ÄŸrenme Fonksiyonu\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return  # Yeterli deneyim yoksa Ã¶ÄŸrenme yapÄ±lmaz.\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)  # HafÄ±zadan rastgele Ã¶rnekler seÃ§ilir.\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if done:\n",
    "                target = reward  # EÄŸer bÃ¶lÃ¼m bittiyse sadece Ã¶dÃ¼l kullanÄ±lÄ±r.\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "                # EÄŸer devam ediyorsa gelecekteki maksimum Ã¶dÃ¼l eklenir.\n",
    "\n",
    "            train_target = self.model.predict(state, verbose=0)\n",
    "            train_target[0][action] = target  # Sadece ilgili aksiyonun Q-deÄŸeri gÃ¼ncellenir.\n",
    "\n",
    "            self.model.fit(state, train_target, verbose=0)  # Model eÄŸitilir.\n",
    "\n",
    "        \n",
    "    # Epsilon GÃ¼ncelleme Fonksiyonu (Exploration AzaltÄ±mÄ±)\n",
    "    def adaptiveEGreedy(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay  # Epsilon zamanla azaltÄ±lÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437fb8ac-2446-4bf1-bfec-b04e16f77091",
   "metadata": {},
   "source": [
    "# âœ… Ana EÄŸitim DÃ¶ngÃ¼sÃ¼ - AjanÄ±n Ortamda Ã–ÄŸrenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52add913-24b6-42b0-aa88-4b4408477379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teymur Mammadov\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\Teymur Mammadov\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:04,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 23 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:42<00:24, 24.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 finished after 11 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:57<00:00, 79.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 finished after 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")  # Ortam oluÅŸturulur (insan gÃ¶zlemi iÃ§in).\n",
    "agent = DQLAgent(env)\n",
    "\n",
    "batch_size = 32  # Mini-batch boyutu.\n",
    "episodes = 3  # EÄŸitim bÃ¶lÃ¼mÃ¼ sayÄ±sÄ± (daha sonra artÄ±rÄ±labilir).\n",
    "\n",
    "for e in tqdm(range(episodes)):\n",
    "    state = env.reset()[0]  # Ortam sÄ±fÄ±rlanÄ±r ve baÅŸlangÄ±Ã§ durumu alÄ±nÄ±r.\n",
    "    state = np.reshape(state, [1, 4])  # Modelle uyumlu hale getirilir.\n",
    "\n",
    "    time = 0  # AdÄ±m sayacÄ±\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)  # Ajan aksiyon seÃ§er.\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated  # BÃ¶lÃ¼mÃ¼n bitip bitmediÄŸi kontrol edilir.\n",
    "        next_state = np.reshape(next_state, [1, 4])  # Modelle uyumlu hale getirilir.\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)  # HafÄ±zaya kaydedilir.\n",
    "        state = next_state  # Yeni duruma geÃ§ilir.\n",
    "\n",
    "        agent.replay(batch_size)  # Ã–ÄŸrenme yapÄ±lÄ±r.\n",
    "        agent.adaptiveEGreedy()  # Epsilon gÃ¼ncellenir.\n",
    "\n",
    "        time += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {e+1} finished after {time} timesteps\")  # BÃ¶lÃ¼m sonucu yazdÄ±rÄ±lÄ±r.\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a6235-55f2-4711-9e8a-7d19d0d39f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the agent learn to balance! ğŸ¤–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
