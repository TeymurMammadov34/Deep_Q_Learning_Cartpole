{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef4a43b-ad57-4400-8779-0181b4271f6f",
   "metadata": {},
   "source": [
    "# âœ…Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5188bc64-b7a8-4570-890a-4c152198b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # NumPy, Ã¶zellikle vektÃ¶r ve matris iÅŸlemleri iÃ§in kullanÄ±lÄ±r.\n",
    "import gym  # OpenAI Gym, RL ortamlarÄ±nÄ± saÄŸlar. Burada CartPole kullanÄ±lÄ±yor.\n",
    "from collections import deque  # Deneyim hafÄ±zasÄ± iÃ§in Ã§ift taraflÄ± kuyruk.\n",
    "from tensorflow.keras.models import Sequential  # Basit katmanlÄ± model tanÄ±mÄ±.\n",
    "from tensorflow.keras.layers import Dense  # Yapay sinir aÄŸÄ±na tam baÄŸlantÄ±lÄ± katmanlar ekler.\n",
    "from tensorflow.keras.optimizers import Adam  # Geri yayÄ±lÄ±mda kullanÄ±lan optimizasyon algoritmasÄ±.\n",
    "import random  # Rastgele eylem seÃ§imi ve Ã¶rnekleme iÃ§in kullanÄ±lÄ±r.\n",
    "from tqdm import tqdm  # EÄŸitim dÃ¶ngÃ¼sÃ¼ ilerlemesini gÃ¶rsel olarak takip etmeye yarar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628cdbb-c6fd-4dfe-88ea-32697c820ef0",
   "metadata": {},
   "source": [
    "# âœ… Derin Q-Learning AjanÄ±nÄ±n TanÄ±mÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3233de3c-3c8e-49d4-80ea-c142174fd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]  # OrtamÄ±n gÃ¶zlem (state) boyutu alÄ±nÄ±r.\n",
    "        self.action_size = env.action_space.n  # Ortamda yapÄ±labilecek toplam eylem sayÄ±sÄ±.\n",
    "\n",
    "        self.gamma = 0.95  # Gelecekteki Ã¶dÃ¼llerin bugÃ¼ne indirgenme katsayÄ±sÄ± (discount factor).\n",
    "        self.learning_rate = 0.001  # Ã–ÄŸrenme oranÄ±.\n",
    "\n",
    "        self.epsilon = 1  # BaÅŸlangÄ±Ã§ta tamamen rastgele eylem seÃ§imi (exploration).\n",
    "        self.epsilon_decay = 0.995  # Her bÃ¶lÃ¼m sonunda epsilon deÄŸeri yavaÅŸÃ§a azaltÄ±lÄ±r.\n",
    "        self.epsilon_min = 0.01  # Epsilonâ€™un ulaÅŸabileceÄŸi en dÃ¼ÅŸÃ¼k deÄŸer.\n",
    "\n",
    "        self.memory = deque(maxlen=1000)  # Deneyimlerin saklanacaÄŸÄ± bir kuyruk (Experience Replay).\n",
    "\n",
    "        self.model = self.build_model()  # Sinir aÄŸÄ± modeli oluÅŸturulur.\n",
    "\n",
    "    # Sinir AÄŸÄ± Modelinin Ä°nÅŸasÄ±\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(48, input_dim=self.state_size, activation=\"relu\"))  # Ä°lk katman: Girdi boyutuna gÃ¶re nÃ¶ronlar.\n",
    "        \n",
    "        model.add(Dense(24, activation=\"relu\"))  # Orta katman: daha az nÃ¶ron ile temsil gÃ¼cÃ¼.\n",
    "       \n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))  # Ã‡Ä±kÄ±ÅŸ katmanÄ±: Her aksiyon iÃ§in bir deÄŸer.\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))  # Hata fonksiyonu ve optimizer.\n",
    "        return model\n",
    "\n",
    "    # HafÄ±zaya Deneyim Ekleme Fonksiyonu\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))  # Her adÄ±mÄ± belleÄŸe kaydeder.\n",
    "\n",
    "    # Aksiyon SeÃ§me Fonksiyonu (Îµ-Greedy PolitikasÄ±)\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return env.action_space.sample()  # Rastgele aksiyon seÃ§ (exploration).\n",
    "        \n",
    "        act_values = self.model.predict(state, verbose=0)  # Modelle tahmin yap (exploitation).\n",
    "        return np.argmax(act_values[0])  # En yÃ¼ksek Q-deÄŸerine sahip aksiyon seÃ§ilir.\n",
    "\n",
    "    # Replay - AjanÄ±n Ã–ÄŸrenme Fonksiyonu\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return  # Yeterli deneyim yoksa Ã¶ÄŸrenme yapÄ±lmaz.\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)  # HafÄ±zadan rastgele Ã¶rnekler seÃ§ilir.\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if done:\n",
    "                target = reward  # EÄŸer bÃ¶lÃ¼m bittiyse sadece Ã¶dÃ¼l kullanÄ±lÄ±r.\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "                # EÄŸer devam ediyorsa gelecekteki maksimum Ã¶dÃ¼l eklenir.\n",
    "\n",
    "            train_target = self.model.predict(state, verbose=0)\n",
    "            train_target[0][action] = target  # Sadece ilgili aksiyonun Q-deÄŸeri gÃ¼ncellenir.\n",
    "\n",
    "            self.model.fit(state, train_target, verbose=0)  # Model eÄŸitilir.\n",
    "\n",
    "        \n",
    "    # Epsilon GÃ¼ncelleme Fonksiyonu (Exploration AzaltÄ±mÄ±)\n",
    "    def adaptiveEGreedy(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay  # Epsilon zamanla azaltÄ±lÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437fb8ac-2446-4bf1-bfec-b04e16f77091",
   "metadata": {},
   "source": [
    "# âœ… Ana EÄŸitim DÃ¶ngÃ¼sÃ¼ - AjanÄ±n Ortamda Ã–ÄŸrenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52add913-24b6-42b0-aa88-4b4408477379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teymur Mammadov\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\Teymur Mammadov\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 18 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [06:35<00:00, 197.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 finished after 44 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")  # Ortam oluÅŸturulur (insan gÃ¶zlemi iÃ§in).\n",
    "agent = DQLAgent(env)\n",
    "\n",
    "batch_size = 32  # Mini-batch boyutu.\n",
    "episodes = 2  # EÄŸitim bÃ¶lÃ¼mÃ¼ sayÄ±sÄ± (daha sonra artÄ±rÄ±labilir).\n",
    "\n",
    "for e in tqdm(range(episodes)):\n",
    "    state = env.reset()[0]  # Ortam sÄ±fÄ±rlanÄ±r ve baÅŸlangÄ±Ã§ durumu alÄ±nÄ±r.\n",
    "    state = np.reshape(state, [1, 4])  # Modelle uyumlu hale getirilir.\n",
    "\n",
    "    time = 0  # AdÄ±m sayacÄ±\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)  # Ajan aksiyon seÃ§er.\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated  # BÃ¶lÃ¼mÃ¼n bitip bitmediÄŸi kontrol edilir.\n",
    "        next_state = np.reshape(next_state, [1, 4])  # Modelle uyumlu hale getirilir.\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)  # HafÄ±zaya kaydedilir.\n",
    "        state = next_state  # Yeni duruma geÃ§ilir.\n",
    "\n",
    "        agent.replay(batch_size)  # Ã–ÄŸrenme yapÄ±lÄ±r.\n",
    "        agent.adaptiveEGreedy()  # Epsilon gÃ¼ncellenir.\n",
    "\n",
    "        time += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {e+1} finished after {time} timesteps\")  # BÃ¶lÃ¼m sonucu yazdÄ±rÄ±lÄ±r.\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627f6b4-7520-45bb-8177-d28b399e9449",
   "metadata": {},
   "source": [
    "# ğŸ§ª EÄŸitilmiÅŸ AjanÄ± Test Etme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703d0450-b3cf-4479-b60b-aa17ef7bf9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1\n",
      "Time: 2\n",
      "Time: 3\n",
      "Time: 4\n",
      "Time: 5\n",
      "Time: 6\n",
      "Time: 7\n",
      "Time: 8\n",
      "Time: 9\n",
      "Time: 10\n",
      "Time: 11\n",
      "Time: 12\n",
      "Time: 13\n",
      "Time: 14\n",
      "Time: 15\n",
      "Time: 16\n",
      "Time: 17\n",
      "Time: 18\n",
      "Time: 19\n",
      "Time: 20\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# EÄŸitilmiÅŸ ajanÄ± kullanÄ±ma hazÄ±r hale getiriyoruz.\n",
    "trained_model = agent\n",
    "\n",
    "# CartPole-v1 ortamÄ±nÄ±, gÃ¶rselleÅŸtirme (render) modunda baÅŸlatÄ±yoruz.\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# OrtamÄ± sÄ±fÄ±rlÄ±yoruz ve baÅŸlangÄ±Ã§ durumunu alÄ±yoruz.\n",
    "# env.reset() fonksiyonu bir tuple dÃ¶ndÃ¼rÃ¼r; ilk eleman gÃ¶zlemlerdir.\n",
    "state = env.reset()[0]\n",
    "\n",
    "# GÃ¶zlem vektÃ¶rÃ¼nÃ¼ (state) modele uygun hale getirmek iÃ§in yeniden ÅŸekillendiriyoruz.\n",
    "# Modelimiz [1, 4] boyutunda giriÅŸ bekliyor.\n",
    "state = np.reshape(state, [1, 4])\n",
    "\n",
    "# Zaman adÄ±mÄ± sayacÄ±nÄ± baÅŸlatÄ±yoruz.\n",
    "time_t = 0\n",
    "\n",
    "# Sonsuz dÃ¶ngÃ¼: AjanÄ± eÄŸitilmiÅŸ politika ile Ã§alÄ±ÅŸtÄ±rÄ±yoruz.\n",
    "while True:\n",
    "    # OrtamÄ± gÃ¶rsel olarak ekranda gÃ¶steriyoruz.\n",
    "    env.render()\n",
    "\n",
    "    # Ajan, mevcut duruma gÃ¶re en iyi eylemi (aksiyonu) tahmin eder.\n",
    "    action = trained_model.act(state)\n",
    "\n",
    "    # Ortamda bu aksiyonu gerÃ§ekleÅŸtiriyoruz; yeni durumu, Ã¶dÃ¼lÃ¼ ve bitiÅŸ bilgisini alÄ±yoruz.\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    # Yeni durumu yine modele uygun forma dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz.\n",
    "    next_state = np.reshape(next_state, [1, 4])\n",
    "\n",
    "    # AjanÄ±n mevcut durumunu gÃ¼ncelliyoruz.\n",
    "    state = next_state\n",
    "\n",
    "    # Zaman adÄ±m sayacÄ±nÄ± artÄ±rÄ±yoruz.\n",
    "    time_t += 1\n",
    "\n",
    "    # Her adÄ±mda geÃ§en zamanÄ± konsola yazdÄ±rÄ±yoruz.\n",
    "    print(f\"Time: {time_t}\")\n",
    "\n",
    "    # AjanÄ±n aksiyonlar arasÄ±nda kÄ±sa bir duraklama yapmasÄ±nÄ± saÄŸlÄ±yoruz.\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # EÄŸer Ã§ubuk dÃ¼ÅŸerse (done=True), dÃ¶ngÃ¼yÃ¼ sonlandÄ±rÄ±yoruz.\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# SimÃ¼lasyon tamamlandÄ±ÄŸÄ±nda mesaj veriyoruz.\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bf005-6ed0-4d6b-9c44-c663f3484011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
